{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6a17586",
   "metadata": {},
   "source": [
    "# Preprocessing of data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec932fe6",
   "metadata": {},
   "source": [
    "Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9d5a413b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "%store -r mortaldict\n",
    "%store -r goddict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f066eb31",
   "metadata": {},
   "source": [
    "First, we import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d1bf626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[313] “So do not thou, my friend, wander long far from home, leaving thy wealth behind thee and men in thy house so insolent, lest they divide and devour all thy wealth, and thou shalt have gone on a fruitless journey. But to Menelaus I bid and command thee to go, for he has but lately come from a strange land, from a folk whence no one would hope in his heart to return, whom the storms had once driven astray into a sea so great, whence the very birds do not fare in the space of a year, so great is it and terrible. But now go thy way with thy ship and thy comrades, or, if thou wilt go by land, here are chariot and horses at hand for thee, and here at thy service are my sons, who will be thy guides to goodly Lacedaemon, where lives fair-haired Menelaus. And do thou beseech him thyself that he may tell thee the very truth. A lie will be not utter, for he is wise indeed.”\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"homer_text_file.txt\",\"r\",encoding='utf-8') as f:\n",
    "    data = f.readlines()\n",
    "    \n",
    "print(data[700])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777c05e5",
   "metadata": {},
   "source": [
    "We split the data into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ded8a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Howbeit Poseidon had gone among the far-off Ethiopians—the Ethiopians who dwell sundered in twain, the farthermost of men, some where Hyperion sets and some where he rises, there to receive a hecatomb of bulls and rams, and there he was taking his joy, sitting at the feast; but the other gods were gathered together in the halls of Olympian Zeus.', \"[28] Among them the father of gods and men was first to speak, for in his heart he thought of noble Aegisthus, whom far-famed Orestes, Agamemnon's son, had slain.\", 'Thinking on him he spoke among the immortals, and said: “Look you now, how ready mortals are to blame the gods.', 'It is from us, they say, that evils come, but they even of themselves, through their own blind folly, have sorrows beyond that which is ordained.', 'Even as now Aegisthus, beyond that which was ordained, took to himself the wedded wife of the son of Atreus, and slew him on his return, though well he knew of sheer destruction, seeing that we spake to him before, sending Hermes, the keen-sighted Argeiphontes, that he should neither slay the man nor woo his wife; for from Orestes shall come vengeance for the son of Atreus when once he has come to manhood and longs for his own land.']\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "for par in data:\n",
    "    if par == '\\n':\n",
    "        continue\n",
    "    senttemp = sent_tokenize(par)\n",
    "    sentences = sentences +senttemp\n",
    "\n",
    "print(sentences[100:105])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7817802b",
   "metadata": {},
   "source": [
    "Now, we tokenize the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6394b7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'heart', ',', 'and', 'as', 'I', 'think', 'it', 'shall', 'be', 'brought', 'to', 'pass', ',', 'though', 'I', 'am', 'in', 'no', 'wise', 'a', 'soothsayer', ',', 'nor', 'one', 'versed', 'in', 'the', 'signs', 'of', 'birds', '.', 'Not', 'much', 'longer', 'shall', 'he', 'be', 'absent', 'from', 'his', 'dear', 'native', 'land', ',', 'no', ',', 'not', 'though', 'bonds', 'of', 'iron', 'hold', 'him', '.', 'He', 'will', 'contrive', 'a', 'way', 'to', 'return', ',', 'for', 'he', 'is', 'a', 'man', 'of', 'many', 'devices', '.', 'But', 'come', ',', 'tell', 'me', 'this', 'and', 'declare', 'it', 'truly', ',', 'whether', 'indeed', ',', 'tall', 'as', 'thou', 'art', ',', 'thou', 'art', 'the', 'son', 'of', 'Odysseus', 'himself', '.', 'Wondrously']\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "for sent in sentences:\n",
    "    wordstemp = word_tokenize(sent)\n",
    "    words = words + wordstemp\n",
    "    \n",
    "print(words[3000:3100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72df7e92",
   "metadata": {},
   "source": [
    "Now, we PoS tag the data, so that lemmatization is easier, and names of characters can be identified more easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e774e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('whether', 'IN'), ('indeed', 'RB'), (',', ','), ('tall', 'JJ'), ('as', 'IN'), ('thou', 'JJ'), ('art', 'NN'), (',', ','), ('thou', 'JJ'), ('art', 'VBP'), ('the', 'DT'), ('son', 'NN'), ('of', 'IN'), ('Odysseus', 'NNP'), ('himself', 'PRP'), ('Wondrously', 'RB'), ('like', 'IN'), ('his', 'PRP$'), ('are', 'VBP'), ('thy', 'JJ'), ('head', 'NN'), ('and', 'CC'), ('beautiful', 'JJ'), ('eyes', 'NNS'), (';', ':'), ('for', 'IN'), ('full', 'JJ'), ('often', 'RB'), ('did', 'VBD'), ('we', 'PRP'), ('consort', 'VB'), ('with', 'IN'), ('one', 'CD'), ('another', 'DT'), ('before', 'IN'), ('he', 'PRP'), ('embarked', 'VBD'), ('for', 'IN'), ('the', 'DT'), ('land', 'NN'), ('of', 'IN'), ('Troy', 'NNP'), (',', ','), ('whither', 'JJR'), ('others', 'NNS'), (',', ','), ('too', 'RB'), (',', ','), ('the', 'DT'), ('bravest', 'JJS'), ('of', 'IN'), ('the', 'DT'), ('Argives', 'NNP'), (',', ','), ('went', 'VBD'), ('in', 'IN'), ('their', 'PRP$'), ('hollow', 'JJ'), ('ships', 'NNS'), ('But', 'CC'), ('since', 'IN'), ('that', 'DT'), ('day', 'NN'), ('neither', 'DT'), ('have', 'VBP'), ('I', 'PRP'), ('seen', 'VBN'), ('Odysseus', 'NNP'), (',', ','), ('nor', 'CC'), ('he', 'PRP'), ('me', 'PRP'), ('”', 'VB'), ('[', 'JJ'), ('213', 'CD'), (']', 'NN'), ('Then', 'RB'), ('wise', 'VB'), ('Telemachus', 'NNP'), ('answered', 'VBD'), ('her', 'PRP$'), (':', ':'), ('“', 'NN'), ('Therefore', 'NNP'), ('of', 'IN'), ('a', 'DT'), ('truth', 'NN'), (',', ','), ('stranger', 'NN'), (',', ','), ('will', 'MD'), ('I', 'PRP'), ('frankly', 'RB'), ('tell', 'VBP'), ('thee', 'RB'), ('all', 'DT'), ('My', 'PRP$'), ('mother', 'NN'), ('says', 'VBZ'), ('that', 'IN')]\n"
     ]
    }
   ],
   "source": [
    "tagged = nltk.pos_tag(words)\n",
    "\n",
    "tagged_filtered = []\n",
    "for wordtag in tagged:\n",
    "    if wordtag[1] == '.':\n",
    "        continue\n",
    "    else: tagged_filtered.append(wordtag)\n",
    "        \n",
    "print(tagged_filtered[3000:3100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55514abb",
   "metadata": {},
   "source": [
    "Next, we lemmatize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "860551d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('whether', 'IN'), ('indeed', 'RB'), (',', ','), ('tall', 'JJ'), ('a', 'IN'), ('thou', 'JJ'), ('art', 'NN'), (',', ','), ('thou', 'JJ'), ('art', 'VBP'), ('the', 'DT'), ('son', 'NN'), ('of', 'IN'), ('Odysseus', 'NNP'), ('himself', 'PRP'), ('Wondrously', 'RB'), ('like', 'IN'), ('his', 'PRP$'), ('be', 'VBP'), ('thy', 'JJ'), ('head', 'NN'), ('and', 'CC'), ('beautiful', 'JJ'), ('eye', 'NNS'), (';', ':'), ('for', 'IN'), ('full', 'JJ'), ('often', 'RB'), ('do', 'VBD'), ('we', 'PRP'), ('consort', 'VB'), ('with', 'IN'), ('one', 'CD'), ('another', 'DT'), ('before', 'IN'), ('he', 'PRP'), ('embark', 'VBD'), ('for', 'IN'), ('the', 'DT'), ('land', 'NN'), ('of', 'IN'), ('Troy', 'NNP'), (',', ','), ('whither', 'JJR'), ('others', 'NNS'), (',', ','), ('too', 'RB'), (',', ','), ('the', 'DT'), ('brave', 'JJS'), ('of', 'IN'), ('the', 'DT'), ('Argives', 'NNP'), (',', ','), ('go', 'VBD'), ('in', 'IN'), ('their', 'PRP$'), ('hollow', 'JJ'), ('ship', 'NNS'), ('But', 'CC'), ('since', 'IN'), ('that', 'DT'), ('day', 'NN'), ('neither', 'DT'), ('have', 'VBP'), ('I', 'PRP'), ('see', 'VBN'), ('Odysseus', 'NNP'), (',', ','), ('nor', 'CC'), ('he', 'PRP'), ('me', 'PRP'), ('”', 'VB'), ('[', 'JJ'), ('213', 'CD'), (']', 'NN'), ('Then', 'RB'), ('wise', 'VB'), ('Telemachus', 'NNP'), ('answer', 'VBD'), ('her', 'PRP$'), (':', ':'), ('“', 'NN'), ('Therefore', 'NNP'), ('of', 'IN'), ('a', 'DT'), ('truth', 'NN'), (',', ','), ('stranger', 'NN'), (',', ','), ('will', 'MD'), ('I', 'PRP'), ('frankly', 'RB'), ('tell', 'VBP'), ('thee', 'RB'), ('all', 'DT'), ('My', 'PRP$'), ('mother', 'NN'), ('say', 'VBZ'), ('that', 'IN')]\n"
     ]
    }
   ],
   "source": [
    "# Reference for lemmatizing: https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatized = []\n",
    "for wordtag in tagged_filtered:\n",
    "    tag = ''\n",
    "    if wordtag[1][0:2] ==  'NN':\n",
    "        tag =  'n'\n",
    "    elif wordtag[1][0:2] ==  'VB':\n",
    "        tag =  'v'\n",
    "    elif wordtag[1][0:2] ==  'JJ':\n",
    "        tag =  'a'\n",
    "    elif wordtag[1][0:2] ==  'RB':\n",
    "        tag =  'r'\n",
    "        \n",
    "    # use postag if it exists in the wordnetlemmatizer\n",
    "    if tag == '': lemma = lemmatizer.lemmatize(wordtag[0])\n",
    "    else: lemma = lemmatizer.lemmatize(wordtag[0], pos=tag)\n",
    "        \n",
    "    lemmatized.append((lemma, wordtag[1]))\n",
    "    \n",
    "print(lemmatized[3000:3100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc24bcd4",
   "metadata": {},
   "source": [
    "Next, we go through the data and print all the proper nouns that aren't in the dictionaries. If they are not and they refer to names of characters that are either Olympian gods or mortals, we add them to the dictionaries in the file characterdicts.ipynb. We do this only for names that occur at least 10 times in the book, since adding all names is not realistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "791b6f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">: 125\n",
      "A: 53\n",
      "A-Z: 25\n",
      "A.: 25\n",
      "ARTICLES: 25\n",
      "Aaron: 25\n",
      "Achaean: 14\n",
      "Achilleid: 25\n",
      "Aeneid: 25\n",
      "Aeschylus: 175\n",
      "Against: 25\n",
      "Ah: 23\n",
      "Alcman: 25\n",
      "Alexandra: 25\n",
      "Antinous: 54\n",
      "Apollodorus: 25\n",
      "Apollonius: 25\n",
      "Aratus: 25\n",
      "Argives: 17\n",
      "Argonautica: 50\n",
      "Argos: 15\n",
      "Astronomica: 25\n",
      "Atsma: 25\n",
      "Autolycus: 18\n",
      "Aye: 34\n",
      "BESTIARY: 25\n",
      "BOOK: 650\n",
      "BY: 25\n",
      "Be: 11\n",
      "Bearers: 25\n",
      "Beggar: 50\n",
      "Bion: 25\n",
      "Book: 25\n",
      "Bound: 25\n",
      "CLASSICAL: 25\n",
      "CONTENTS: 25\n",
      "Callimachus: 25\n",
      "Callistratus: 25\n",
      "Calypso: 50\n",
      "Chiliades: 25\n",
      "Circe: 70\n",
      "Classical: 52\n",
      "Claudian: 50\n",
      "Clement: 50\n",
      "Colluthus: 25\n",
      "Come: 24\n",
      "Contact: 25\n",
      "Contest: 50\n",
      "Copyright: 25\n",
      "Creatures: 25\n",
      "Crete: 14\n",
      "Cretensis: 25\n",
      "Cronos: 34\n",
      "Cycle: 25\n",
      "Cyclopes: 12\n",
      "Cyclops: 45\n",
      "Daemones-Spirits: 25\n",
      "Dares: 25\n",
      "Dawn: 30\n",
      "Days: 25\n",
      "Dead: 50\n",
      "Dear: 11\n",
      "Departure: 25\n",
      "Description: 25\n",
      "Descriptions: 25\n",
      "Dialogues: 50\n",
      "Dictys: 25\n",
      "Diodorus: 25\n",
      "Dionysiaca: 25\n",
      "Eclogues: 25\n",
      "Egypt: 13\n",
      "Elder: 25\n",
      "Epic: 25\n",
      "Eumenides: 25\n",
      "Exhortation: 25\n",
      "Fabulae: 25\n",
      "Fall: 25\n",
      "Family: 25\n",
      "Fasti: 25\n",
      "Father: 23\n",
      "Feast: 25\n",
      "First: 15\n",
      "Flaccus: 25\n",
      "Flower: 25\n",
      "Fragments: 125\n",
      "Friend: 11\n",
      "Fulgentius: 25\n",
      "Furens: 25\n",
      "GALLERY: 25\n",
      "GODS: 25\n",
      "GREEK: 50\n",
      "Games: 25\n",
      "Georgics: 25\n",
      "Gerenia: 11\n",
      "Gigantomachia: 25\n",
      "Gods: 275\n",
      "Greece: 25\n",
      "Greek: 27\n",
      "Greeks: 27\n",
      "HEROES: 25\n",
      "HOME: 25\n",
      "HOMER: 52\n",
      "Helen: 46\n",
      "Heroides: 25\n",
      "Hesiod: 100\n",
      "Him: 15\n",
      "History: 25\n",
      "Homer: 77\n",
      "Homeric: 25\n",
      "Howbeit: 17\n",
      "Hyginus: 50\n",
      "Hymns: 75\n",
      "Idylls: 25\n",
      "Iliad: 27\n",
      "Ilios: 43\n",
      "Imagines: 50\n",
      "Irus: 12\n",
      "Ithaca: 129\n",
      "J.: 25\n",
      "LIBRARY: 50\n",
      "Laestrygones: 25\n",
      "Legendary: 50\n",
      "Libation: 25\n",
      "Library: 127\n",
      "Life: 25\n",
      "Lo: 22\n",
      "Lotus-Eaters: 25\n",
      "Love: 25\n",
      "Lucian: 50\n",
      "Lycophron: 25\n",
      "Lyric: 25\n",
      "MISCELLANY: 25\n",
      "MURRAY: 25\n",
      "MYTHOLOGY: 25\n",
      "May: 10\n",
      "Medea: 25\n",
      "Medon: 10\n",
      "Mentor: 20\n",
      "Metamorphoses: 25\n",
      "Monsters: 50\n",
      "Moschus: 25\n",
      "My: 20\n",
      "Mythologies: 25\n",
      "Myths: 50\n",
      "Nay: 122\n",
      "Neleus: 12\n",
      "New: 25\n",
      "Nonnus: 25\n",
      "North: 13\n",
      "Nymphs: 27\n",
      "O: 26\n",
      "ODYSSEY: 100\n",
      "Oceanus: 14\n",
      "Odyssey: 56\n",
      "Oetaeus: 25\n",
      "Old: 12\n",
      "Olympian: 30\n",
      "Olympus: 21\n",
      "Orphic: 25\n",
      "Ovid: 75\n",
      "Parallel: 25\n",
      "Parthenius: 25\n",
      "Pattern: 25\n",
      "Pausanias: 25\n",
      "Phaenomena: 25\n",
      "Philostratus: 50\n",
      "Phoenissae: 25\n",
      "Phrygius: 25\n",
      "Plant: 25\n",
      "Plutarch: 50\n",
      "Poems: 75\n",
      "Primordial: 25\n",
      "Project: 50\n",
      "Prometheus: 25\n",
      "Proserpine: 25\n",
      "Pylos: 41\n",
      "Quintus: 25\n",
      "RECENT: 25\n",
      "Raft: 25\n",
      "Rape: 50\n",
      "Recognitions: 25\n",
      "Return: 50\n",
      "Returns: 50\n",
      "Rhodius: 25\n",
      "Romances: 25\n",
      "Rustic: 25\n",
      "SEARCH: 25\n",
      "Scylla: 37\n",
      "Sea: 25\n",
      "Seneca: 225\n",
      "Seven: 25\n",
      "Shield: 25\n",
      "Siculus: 25\n",
      "Sky: 25\n",
      "Slaying: 25\n",
      "Smyrnaeus: 25\n",
      "So: 89\n",
      "Soli: 25\n",
      "Son: 13\n",
      "Sparta: 10\n",
      "Spirits: 50\n",
      "Star: 25\n",
      "Statius: 50\n",
      "Stories: 25\n",
      "Straightway: 20\n",
      "Stranger: 25\n",
      "Suitors: 75\n",
      "Suppliant: 25\n",
      "T.: 27\n",
      "TEXTS: 25\n",
      "THE: 25\n",
      "THEOI: 25\n",
      "Taking: 25\n",
      "Texts: 75\n",
      "Thebaid: 25\n",
      "Thebes: 26\n",
      "Theocritus: 25\n",
      "Theogony: 25\n",
      "Theoi: 75\n",
      "Therefore: 10\n",
      "Therein: 10\n",
      "Thou: 34\n",
      "Thus: 26\n",
      "Titan: 25\n",
      "Toggle: 25\n",
      "Tree: 25\n",
      "Tribes: 25\n",
      "Troades: 25\n",
      "Troy: 68\n",
      "Tryphiodorus: 25\n",
      "Tzetzes: 25\n",
      "Underworld: 50\n",
      "Us: 25\n",
      "Valerius: 25\n",
      "Verily: 14\n",
      "Virgil: 75\n",
      "Warrior: 25\n",
      "Was: 26\n",
      "West: 12\n",
      "Who: 26\n",
      "Wind: 32\n",
      "Women: 25\n",
      "Works: 25\n",
      "Younger: 25\n",
      "Zealand: 25\n",
      "]: 68\n",
      "©: 25\n",
      "“: 80\n",
      "”: 12\n"
     ]
    }
   ],
   "source": [
    "# Reference for sorting dictionary: https://www.edureka.co/blog/sort-dictionary-by-value-in-python/\n",
    "\n",
    "non_occuring = {}\n",
    "non_occuring_min10 = {}\n",
    "\n",
    "mortals = mortaldict\n",
    "gods = goddict\n",
    "\n",
    "for wordtag in lemmatized:\n",
    "    \n",
    "    # Only look at proper nouns\n",
    "    if wordtag[1] == 'NNP':\n",
    "        \n",
    "        # Binary to indicate if the word has been found\n",
    "        filled_in = 0\n",
    "        \n",
    "        # Go through mortaldict\n",
    "        for idx, vals in enumerate(mortals.values()):\n",
    "            if wordtag[0] in vals[0]:\n",
    "                filled_in = 1\n",
    "                break\n",
    "                \n",
    "        # If word hasn't been found in mortaldict, go through goddict\n",
    "        if filled_in == 0:\n",
    "            for idx, vals in enumerate(gods.values()):\n",
    "                if wordtag[0] in vals[0]:\n",
    "                    filled_in = 1\n",
    "                    break\n",
    "        \n",
    "        # Print word if it hasn't been found at all\n",
    "        if filled_in == 0:\n",
    "            if wordtag[0] in non_occuring.keys():\n",
    "                non_occuring[wordtag[0]] += 1\n",
    "            else: non_occuring[wordtag[0]] = 1\n",
    "\n",
    "for key in non_occuring.keys():\n",
    "    if non_occuring[key] >= 10:\n",
    "        non_occuring_min10[key] = non_occuring[key]\n",
    "\n",
    "for key in sorted(non_occuring_min10.keys()):\n",
    "       print(\"%s: %s\" % (key, non_occuring_min10[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d203b5c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
